{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1yVa8kHyZj2jORC5YAg3A_6PKt5N_4KNB","timestamp":1708904790146},{"file_id":"15ncw8X_c9Rkbtqqy50lY8KtJCA2D_gfy","timestamp":1708838954610}],"authorship_tag":"ABX9TyMrjCOvC7tlOgfWRKRWVYLv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["In this assessment, you'll continue working on the MNIST dataset. So, train and test the models specified in the tasks below using the MNIST data. You can choose the number of epochs for training. But for the sake of comparison, it's a good idea to train for 20 epochs.\n","\n","To complete this assessment, create a Jupyter Notebook containing your solutions to the following tasks:\n","\n","In this task, you'll implement several ANN models with different batch sizes. Specifically, do the following:\n","\n","Implement a three-layer ANN model with 128, 64, and 10 neurons in the layers. Use 8 as the mini-batch size.\n","Implement a three-layer ANN model with 128, 64, and 10 neurons in the layers. Use 128 as the mini-batch size.\n","Implement a three-layer ANN model with 128, 64, and 10 neurons in the layers. Use the full sample as the batch size.\n","Compare the results of each model. Which batch size performed best?"],"metadata":{"id":"dDNmG7fRC9Pj"}},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from tensorflow.keras.datasets import mnist"],"metadata":{"id":"LyTwtHezBbwD","executionInfo":{"status":"ok","timestamp":1708904858118,"user_tz":360,"elapsed":3606,"user":{"displayName":"U S","userId":"13531373332520883304"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","\n","input_dim = 784  # 28*28\n","output_dim = nb_classes = 10\n","batch_size = 128\n","nb_epoch = 20\n","\n","X_train = X_train.reshape(60000, input_dim)\n","X_test = X_test.reshape(10000, input_dim)\n","X_train = X_train.astype('float32')\n","X_test = X_test.astype('float32')\n","X_train /= 255\n","X_test /= 255"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-c3ZV6olBgHg","executionInfo":{"status":"ok","timestamp":1708904859014,"user_tz":360,"elapsed":902,"user":{"displayName":"U S","userId":"13531373332520883304"}},"outputId":"af18ef94-ae5a-4f63-9446-6f7d0d62d152"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.utils import to_categorical\n","\n","Y_train = to_categorical(y_train, nb_classes)\n","Y_test = to_categorical(y_test, nb_classes)"],"metadata":{"id":"zpKGEvAJBjH1","executionInfo":{"status":"ok","timestamp":1708904862248,"user_tz":360,"elapsed":191,"user":{"displayName":"U S","userId":"13531373332520883304"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["X_train[0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7WbVEZ2PBldp","executionInfo":{"status":"ok","timestamp":1708904864568,"user_tz":360,"elapsed":8,"user":{"displayName":"U S","userId":"13531373332520883304"}},"outputId":"192208bc-a18d-4dc3-e5e3-f4feb0408ac2"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(784,)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(20,5))\n","\n","plt.subplot(141)\n","plt.imshow(X_train[123].reshape(28,28), cmap=\"gray\")\n","plt.title(\"Label of the image: {}\".format(y_train[123]))\n","\n","plt.subplot(142)\n","plt.imshow(X_train[124].reshape(28,28), cmap=\"gray\")\n","plt.title(\"Label of the image: {}\".format(y_train[124]))\n","\n","plt.subplot(143)\n","plt.imshow(X_train[125].reshape(28,28), cmap=\"gray\")\n","plt.title(\"Label of the image: {}\".format(y_train[125]))\n","\n","plt.subplot(144)\n","plt.imshow(X_train[126].reshape(28,28), cmap=\"gray\")\n","plt.title(\"Label of the image: {}\".format(y_train[126]))\n","\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":420},"id":"0npXVXmjBnPY","executionInfo":{"status":"ok","timestamp":1708904869238,"user_tz":360,"elapsed":653,"user":{"displayName":"U S","userId":"13531373332520883304"}},"outputId":"04f34a49-b3ef-494f-96b7-cbcf5595328f"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 2000x500 with 4 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Activation, Dense\n","from keras import backend as K\n","from tensorflow.keras import losses\n","\n","batch_size = 8\n","\n","model = Sequential()\n","# The first dense layer\n","model.add(Dense(128, input_shape=(784,), activation='relu'))\n","# The second dense layer\n","model.add(Dense(64, activation='relu'))\n","# The last layer is the output layer\n","model.add(Dense(10, activation='softmax'))\n","\n","model.summary()\n","\n","model.compile(optimizer='sgd', loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Setting `verbose=1` prints out some results after each epoch\n","model.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)\n","\n","score = model.evaluate(X_test, Y_test, verbose=0)\n","print('Test score:', score[0])\n","print('Test accuracy:', score[1])"],"metadata":{"id":"vt7glvYsBttM","executionInfo":{"status":"ok","timestamp":1708905167283,"user_tz":360,"elapsed":208677,"user":{"displayName":"U S","userId":"13531373332520883304"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a4e6cb59-3bd3-40d6-e89a-650ccb664468"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense (Dense)               (None, 128)               100480    \n","                                                                 \n"," dense_1 (Dense)             (None, 64)                8256      \n","                                                                 \n"," dense_2 (Dense)             (None, 10)                650       \n","                                                                 \n","=================================================================\n","Total params: 109386 (427.29 KB)\n","Trainable params: 109386 (427.29 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/20\n","7500/7500 [==============================] - 11s 1ms/step - loss: 0.3627 - accuracy: 0.8964\n","Epoch 2/20\n","7500/7500 [==============================] - 11s 2ms/step - loss: 0.1676 - accuracy: 0.9512\n","Epoch 3/20\n","7500/7500 [==============================] - 10s 1ms/step - loss: 0.1224 - accuracy: 0.9643\n","Epoch 4/20\n","7500/7500 [==============================] - 10s 1ms/step - loss: 0.0967 - accuracy: 0.9717\n","Epoch 5/20\n","7500/7500 [==============================] - 10s 1ms/step - loss: 0.0789 - accuracy: 0.9771\n","Epoch 6/20\n","7500/7500 [==============================] - 13s 2ms/step - loss: 0.0671 - accuracy: 0.9800\n","Epoch 7/20\n","7500/7500 [==============================] - 11s 1ms/step - loss: 0.0571 - accuracy: 0.9829\n","Epoch 8/20\n","7500/7500 [==============================] - 10s 1ms/step - loss: 0.0482 - accuracy: 0.9857\n","Epoch 9/20\n","7500/7500 [==============================] - 10s 1ms/step - loss: 0.0417 - accuracy: 0.9880\n","Epoch 10/20\n","7500/7500 [==============================] - 10s 1ms/step - loss: 0.0371 - accuracy: 0.9895\n","Epoch 11/20\n","7500/7500 [==============================] - 10s 1ms/step - loss: 0.0314 - accuracy: 0.9909\n","Epoch 12/20\n","7500/7500 [==============================] - 10s 1ms/step - loss: 0.0269 - accuracy: 0.9927\n","Epoch 13/20\n","7500/7500 [==============================] - 10s 1ms/step - loss: 0.0237 - accuracy: 0.9934\n","Epoch 14/20\n","7500/7500 [==============================] - 9s 1ms/step - loss: 0.0200 - accuracy: 0.9946\n","Epoch 15/20\n","7500/7500 [==============================] - 10s 1ms/step - loss: 0.0172 - accuracy: 0.9957\n","Epoch 16/20\n","7500/7500 [==============================] - 11s 1ms/step - loss: 0.0146 - accuracy: 0.9966\n","Epoch 17/20\n","7500/7500 [==============================] - 10s 1ms/step - loss: 0.0127 - accuracy: 0.9973\n","Epoch 18/20\n","7500/7500 [==============================] - 10s 1ms/step - loss: 0.0110 - accuracy: 0.9978\n","Epoch 19/20\n","7500/7500 [==============================] - 10s 1ms/step - loss: 0.0089 - accuracy: 0.9986\n","Epoch 20/20\n","7500/7500 [==============================] - 10s 1ms/step - loss: 0.0080 - accuracy: 0.9987\n","Test score: 0.07431649416685104\n","Test accuracy: 0.9789000153541565\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Activation, Dense\n","from keras import backend as K\n","from tensorflow.keras import losses\n","\n","batch_size = 128\n","\n","model = Sequential()\n","# The first dense layer\n","model.add(Dense(128, input_shape=(784,), activation='relu'))\n","# The second dense layer\n","model.add(Dense(64, activation='relu'))\n","# The last layer is the output layer\n","model.add(Dense(10, activation='softmax'))\n","\n","model.summary()\n","\n","model.compile(optimizer='sgd', loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Setting `verbose=1` prints out some results after each epoch\n","model.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)\n","\n","score = model.evaluate(X_test, Y_test, verbose=0)\n","print('Test score:', score[0])\n","print('Test accuracy:', score[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"heH-ET58rJBN","executionInfo":{"status":"ok","timestamp":1708905309227,"user_tz":360,"elapsed":42446,"user":{"displayName":"U S","userId":"13531373332520883304"}},"outputId":"c8211e9d-3989-49ae-aaa3-d23ab305e471"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_3 (Dense)             (None, 128)               100480    \n","                                                                 \n"," dense_4 (Dense)             (None, 64)                8256      \n","                                                                 \n"," dense_5 (Dense)             (None, 10)                650       \n","                                                                 \n","=================================================================\n","Total params: 109386 (427.29 KB)\n","Trainable params: 109386 (427.29 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/20\n","469/469 [==============================] - 1s 2ms/step - loss: 1.2800 - accuracy: 0.6663\n","Epoch 2/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.5112 - accuracy: 0.8659\n","Epoch 3/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.3890 - accuracy: 0.8935\n","Epoch 4/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.3392 - accuracy: 0.9047\n","Epoch 5/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.3091 - accuracy: 0.9121\n","Epoch 6/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2879 - accuracy: 0.9174\n","Epoch 7/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2709 - accuracy: 0.9227\n","Epoch 8/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2566 - accuracy: 0.9268\n","Epoch 9/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2440 - accuracy: 0.9300\n","Epoch 10/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2328 - accuracy: 0.9334\n","Epoch 11/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2232 - accuracy: 0.9359\n","Epoch 12/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2145 - accuracy: 0.9383\n","Epoch 13/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2061 - accuracy: 0.9412\n","Epoch 14/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1985 - accuracy: 0.9432\n","Epoch 15/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.1919 - accuracy: 0.9450\n","Epoch 16/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.1852 - accuracy: 0.9470\n","Epoch 17/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1792 - accuracy: 0.9484\n","Epoch 18/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1735 - accuracy: 0.9504\n","Epoch 19/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1681 - accuracy: 0.9519\n","Epoch 20/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1632 - accuracy: 0.9531\n","Test score: 0.16484515368938446\n","Test accuracy: 0.9502999782562256\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Activation, Dense\n","from keras import backend as K\n","from tensorflow.keras import losses\n","\n","batch_size = X_train.shape[0]\n","\n","model = Sequential()\n","# The first dense layer\n","model.add(Dense(128, input_shape=(784,), activation='relu'))\n","# The second dense layer\n","model.add(Dense(64, activation='relu'))\n","# The last layer is the output layer\n","model.add(Dense(10, activation='softmax'))\n","\n","model.summary()\n","\n","model.compile(optimizer='sgd', loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Setting `verbose=1` prints out some results after each epoch\n","model.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)\n","\n","score = model.evaluate(X_test, Y_test, verbose=0)\n","print('Test score:', score[0])\n","print('Test accuracy:', score[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tn2qbRwSyEvI","executionInfo":{"status":"ok","timestamp":1708905547710,"user_tz":360,"elapsed":11871,"user":{"displayName":"U S","userId":"13531373332520883304"}},"outputId":"e1e71f93-6372-4b30-a4aa-fd7a3366c00c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_9 (Dense)             (None, 128)               100480    \n","                                                                 \n"," dense_10 (Dense)            (None, 64)                8256      \n","                                                                 \n"," dense_11 (Dense)            (None, 10)                650       \n","                                                                 \n","=================================================================\n","Total params: 109386 (427.29 KB)\n","Trainable params: 109386 (427.29 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/20\n","1/1 [==============================] - 1s 1s/step - loss: 2.2966 - accuracy: 0.1511\n","Epoch 2/20\n","1/1 [==============================] - 0s 443ms/step - loss: 2.2893 - accuracy: 0.1564\n","Epoch 3/20\n","1/1 [==============================] - 0s 422ms/step - loss: 2.2822 - accuracy: 0.1621\n","Epoch 4/20\n","1/1 [==============================] - 0s 436ms/step - loss: 2.2752 - accuracy: 0.1677\n","Epoch 5/20\n","1/1 [==============================] - 0s 422ms/step - loss: 2.2685 - accuracy: 0.1736\n","Epoch 6/20\n","1/1 [==============================] - 0s 430ms/step - loss: 2.2619 - accuracy: 0.1798\n","Epoch 7/20\n","1/1 [==============================] - 0s 424ms/step - loss: 2.2554 - accuracy: 0.1859\n","Epoch 8/20\n","1/1 [==============================] - 0s 434ms/step - loss: 2.2491 - accuracy: 0.1919\n","Epoch 9/20\n","1/1 [==============================] - 0s 424ms/step - loss: 2.2429 - accuracy: 0.1985\n","Epoch 10/20\n","1/1 [==============================] - 0s 426ms/step - loss: 2.2369 - accuracy: 0.2048\n","Epoch 11/20\n","1/1 [==============================] - 0s 426ms/step - loss: 2.2309 - accuracy: 0.2110\n","Epoch 12/20\n","1/1 [==============================] - 0s 426ms/step - loss: 2.2250 - accuracy: 0.2174\n","Epoch 13/20\n","1/1 [==============================] - 0s 433ms/step - loss: 2.2192 - accuracy: 0.2238\n","Epoch 14/20\n","1/1 [==============================] - 0s 421ms/step - loss: 2.2134 - accuracy: 0.2302\n","Epoch 15/20\n","1/1 [==============================] - 1s 502ms/step - loss: 2.2078 - accuracy: 0.2368\n","Epoch 16/20\n","1/1 [==============================] - 1s 593ms/step - loss: 2.2022 - accuracy: 0.2426\n","Epoch 17/20\n","1/1 [==============================] - 1s 579ms/step - loss: 2.1966 - accuracy: 0.2490\n","Epoch 18/20\n","1/1 [==============================] - 0s 404ms/step - loss: 2.1911 - accuracy: 0.2553\n","Epoch 19/20\n","1/1 [==============================] - 0s 380ms/step - loss: 2.1857 - accuracy: 0.2607\n","Epoch 20/20\n","1/1 [==============================] - 0s 385ms/step - loss: 2.1803 - accuracy: 0.2673\n","Test score: 2.1709771156311035\n","Test accuracy: 0.28060001134872437\n"]}]},{"cell_type":"markdown","source":["In this task, you'll implement several ANN models with different learning rates for the stochastic gradient descent. In all of the models below, use 128 as your mini-batch size.\n","\n","Implement a three-layer ANN model with 128, 64, and 10 neurons in the layers. Use 0.01 as the learning rate.\n","Implement a three-layer ANN model with 128, 64, and 10 neurons in the layers. Use 100 as the learning rate.\n","Implement a three-layer ANN model with 128, 64, and 10 neurons in the layers. Use 0.0000001 as the learning rate.\n","Compare the results of each model. Which learning rate performed best?"],"metadata":{"id":"MwOb_x0etjEa"}},{"cell_type":"code","source":["from tensorflow.keras import optimizers\n","sgd_001 = optimizers.SGD(lr=0.01)\n","sgd_100 = optimizers.SGD(lr=100)\n","sgd_00000001 = optimizers.SGD(lr=0.0000001)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YVXxuacAtlp4","executionInfo":{"status":"ok","timestamp":1708907890206,"user_tz":360,"elapsed":145,"user":{"displayName":"U S","userId":"13531373332520883304"}},"outputId":"982c58a9-ad35-4edd-96da-6935387eeead"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n","WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n","WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Activation, Dense\n","from keras import backend as K\n","from tensorflow.keras import losses\n","\n","batch_size = 128\n","\n","model = Sequential()\n","# The first dense layer\n","model.add(Dense(128, input_shape=(784,), activation='relu'))\n","# The second dense layer\n","model.add(Dense(64, activation='relu'))\n","# The last layer is the output layer\n","model.add(Dense(10, activation='softmax'))\n","\n","model.summary()\n","\n","model.compile(optimizer=sgd_001, loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Setting `verbose=1` prints out some results after each epoch\n","model.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)\n","\n","score = model.evaluate(X_test, Y_test, verbose=0)\n","print('Test score:', score[0])\n","print('Test accuracy:', score[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rksuG1ZuyE7v","executionInfo":{"status":"ok","timestamp":1708907933884,"user_tz":360,"elapsed":42380,"user":{"displayName":"U S","userId":"13531373332520883304"}},"outputId":"6d19cdcd-bbde-4c17-e8de-5e3969a1f9bc"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_9\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_27 (Dense)            (None, 128)               100480    \n","                                                                 \n"," dense_28 (Dense)            (None, 64)                8256      \n","                                                                 \n"," dense_29 (Dense)            (None, 10)                650       \n","                                                                 \n","=================================================================\n","Total params: 109386 (427.29 KB)\n","Trainable params: 109386 (427.29 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/20\n","469/469 [==============================] - 2s 3ms/step - loss: 1.2853 - accuracy: 0.6726\n","Epoch 2/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.8658\n","Epoch 3/20\n","469/469 [==============================] - 2s 3ms/step - loss: 0.3952 - accuracy: 0.8901\n","Epoch 4/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.3474 - accuracy: 0.9023\n","Epoch 5/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.3175 - accuracy: 0.9094\n","Epoch 6/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2961 - accuracy: 0.9148\n","Epoch 7/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2790 - accuracy: 0.9197\n","Epoch 8/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2646 - accuracy: 0.9242\n","Epoch 9/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2518 - accuracy: 0.9281\n","Epoch 10/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.2407 - accuracy: 0.9312\n","Epoch 11/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2307 - accuracy: 0.9342\n","Epoch 12/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.2213 - accuracy: 0.9368\n","Epoch 13/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.2129 - accuracy: 0.9398\n","Epoch 14/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2051 - accuracy: 0.9416\n","Epoch 15/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1980 - accuracy: 0.9437\n","Epoch 16/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1910 - accuracy: 0.9456\n","Epoch 17/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1846 - accuracy: 0.9471\n","Epoch 18/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1785 - accuracy: 0.9490\n","Epoch 19/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1731 - accuracy: 0.9505\n","Epoch 20/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1677 - accuracy: 0.9518\n","Test score: 0.16540342569351196\n","Test accuracy: 0.9513999819755554\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Activation, Dense\n","from keras import backend as K\n","from tensorflow.keras import losses\n","\n","batch_size = 128\n","\n","model = Sequential()\n","# The first dense layer\n","model.add(Dense(128, input_shape=(784,), activation='relu'))\n","# The second dense layer\n","model.add(Dense(64, activation='relu'))\n","# The last layer is the output layer\n","model.add(Dense(10, activation='softmax'))\n","\n","model.summary()\n","\n","model.compile(optimizer=sgd_100, loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Setting `verbose=1` prints out some results after each epoch\n","model.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)\n","\n","score = model.evaluate(X_test, Y_test, verbose=0)\n","print('Test score:', score[0])\n","print('Test accuracy:', score[1])"],"metadata":{"id":"o6IY94QUDAvL","executionInfo":{"status":"ok","timestamp":1708905811102,"user_tz":360,"elapsed":33107,"user":{"displayName":"U S","userId":"13531373332520883304"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"22802c7e-63bb-4862-a389-6182d3c61097"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_15 (Dense)            (None, 128)               100480    \n","                                                                 \n"," dense_16 (Dense)            (None, 64)                8256      \n","                                                                 \n"," dense_17 (Dense)            (None, 10)                650       \n","                                                                 \n","=================================================================\n","Total params: 109386 (427.29 KB)\n","Trainable params: 109386 (427.29 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/20\n","469/469 [==============================] - 2s 3ms/step - loss: 1.1901 - accuracy: 0.6992\n","Epoch 2/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.4951 - accuracy: 0.8699\n","Epoch 3/20\n","469/469 [==============================] - 2s 4ms/step - loss: 0.3886 - accuracy: 0.8918\n","Epoch 4/20\n","469/469 [==============================] - 3s 5ms/step - loss: 0.3425 - accuracy: 0.9026\n","Epoch 5/20\n","469/469 [==============================] - 2s 5ms/step - loss: 0.3133 - accuracy: 0.9110\n","Epoch 6/20\n","469/469 [==============================] - 2s 5ms/step - loss: 0.2923 - accuracy: 0.9166\n","Epoch 7/20\n","469/469 [==============================] - 2s 5ms/step - loss: 0.2757 - accuracy: 0.9209\n","Epoch 8/20\n","469/469 [==============================] - 2s 4ms/step - loss: 0.2616 - accuracy: 0.9254\n","Epoch 9/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.2494 - accuracy: 0.9284\n","Epoch 10/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2387 - accuracy: 0.9309\n","Epoch 11/20\n","469/469 [==============================] - 2s 4ms/step - loss: 0.2291 - accuracy: 0.9340\n","Epoch 12/20\n","469/469 [==============================] - 2s 4ms/step - loss: 0.2206 - accuracy: 0.9366\n","Epoch 13/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2126 - accuracy: 0.9383\n","Epoch 14/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2055 - accuracy: 0.9410\n","Epoch 15/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1987 - accuracy: 0.9429\n","Epoch 16/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1925 - accuracy: 0.9447\n","Epoch 17/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1868 - accuracy: 0.9459\n","Epoch 18/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.1811 - accuracy: 0.9477\n","Epoch 19/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.1762 - accuracy: 0.9494\n","Epoch 20/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.1714 - accuracy: 0.9507\n","Test score: 0.16706658899784088\n","Test accuracy: 0.9513999819755554\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Activation, Dense\n","from keras import backend as K\n","from tensorflow.keras import losses\n","\n","batch_size = 128\n","\n","model = Sequential()\n","# The first dense layer\n","model.add(Dense(128, input_shape=(784,), activation='relu'))\n","# The second dense layer\n","model.add(Dense(64, activation='relu'))\n","# The last layer is the output layer\n","model.add(Dense(10, activation='softmax'))\n","\n","model.summary()\n","\n","model.compile(optimizer=sgd_001, loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Setting `verbose=1` prints out some results after each epoch\n","model.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)\n","\n","score = model.evaluate(X_test, Y_test, verbose=0)\n","print('Test score:', score[0])\n","print('Test accuracy:', score[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qdPRcDsuDdaf","executionInfo":{"status":"ok","timestamp":1708906387602,"user_tz":360,"elapsed":42794,"user":{"displayName":"U S","userId":"13531373332520883304"}},"outputId":"b0b85de1-6ff4-42ca-a370-0695809f75b4"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_6\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_18 (Dense)            (None, 128)               100480    \n","                                                                 \n"," dense_19 (Dense)            (None, 64)                8256      \n","                                                                 \n"," dense_20 (Dense)            (None, 10)                650       \n","                                                                 \n","=================================================================\n","Total params: 109386 (427.29 KB)\n","Trainable params: 109386 (427.29 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/20\n","469/469 [==============================] - 2s 2ms/step - loss: 1.2026 - accuracy: 0.7034\n","Epoch 2/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.4820 - accuracy: 0.8745\n","Epoch 3/20\n","469/469 [==============================] - 1s 3ms/step - loss: 0.3789 - accuracy: 0.8955\n","Epoch 4/20\n","469/469 [==============================] - 2s 3ms/step - loss: 0.3345 - accuracy: 0.9056\n","Epoch 5/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.3068 - accuracy: 0.9122\n","Epoch 6/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2867 - accuracy: 0.9181\n","Epoch 7/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2700 - accuracy: 0.9223\n","Epoch 8/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2560 - accuracy: 0.9269\n","Epoch 9/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2442 - accuracy: 0.9304\n","Epoch 10/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2337 - accuracy: 0.9330\n","Epoch 11/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2245 - accuracy: 0.9360\n","Epoch 12/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2159 - accuracy: 0.9384\n","Epoch 13/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.2082 - accuracy: 0.9403\n","Epoch 14/20\n","469/469 [==============================] - 2s 3ms/step - loss: 0.2010 - accuracy: 0.9421\n","Epoch 15/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1944 - accuracy: 0.9441\n","Epoch 16/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1885 - accuracy: 0.9454\n","Epoch 17/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1828 - accuracy: 0.9474\n","Epoch 18/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1772 - accuracy: 0.9491\n","Epoch 19/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1722 - accuracy: 0.9502\n","Epoch 20/20\n","469/469 [==============================] - 1s 2ms/step - loss: 0.1676 - accuracy: 0.9515\n","Test score: 0.16697202622890472\n","Test accuracy: 0.9506000280380249\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Activation, Dense\n","from keras import backend as K\n","from tensorflow.keras import losses\n","\n","batch_size = 128\n","\n","model = Sequential()\n","# The first dense layer\n","model.add(Dense(128, input_shape=(784,), activation='relu'))\n","# The second dense layer\n","model.add(Dense(64, activation='relu'))\n","# The last layer is the output layer\n","model.add(Dense(10, activation='softmax'))\n","\n","model.summary()\n","\n","model.compile(optimizer=sgd_00000001, loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Setting `verbose=1` prints out some results after each epoch\n","model.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)\n","\n","score = model.evaluate(X_test, Y_test, verbose=0)\n","print('Test score:', score[0])\n","print('Test accuracy:', score[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"3Kg3wpwD1m2n","executionInfo":{"status":"error","timestamp":1708907859914,"user_tz":360,"elapsed":462,"user":{"displayName":"U S","userId":"13531373332520883304"}},"outputId":"073b220c-5804-47f9-c84e-a3cb7c1f2f0d"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_8\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_24 (Dense)            (None, 128)               100480    \n","                                                                 \n"," dense_25 (Dense)            (None, 64)                8256      \n","                                                                 \n"," dense_26 (Dense)            (None, 10)                650       \n","                                                                 \n","=================================================================\n","Total params: 109386 (427.29 KB)\n","Trainable params: 109386 (427.29 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/20\n"]},{"output_type":"error","ename":"KeyError","evalue":"in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1154, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 544, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1223, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 652, in apply_gradients\n        iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1253, in _internal_apply_gradients\n        return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1345, in _distributed_apply_gradients_fn\n        distribution.extended.update(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1342, in apply_grad_to_update_var  **\n        return self._update_step(grad, var)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 233, in _update_step\n        raise KeyError(\n\n    KeyError: 'The optimizer cannot recognize variable dense_24/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.SGD.'\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-b78e8cc190e2>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Setting `verbose=1` prints out some results after each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1154, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 544, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1223, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 652, in apply_gradients\n        iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1253, in _internal_apply_gradients\n        return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1345, in _distributed_apply_gradients_fn\n        distribution.extended.update(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 1342, in apply_grad_to_update_var  **\n        return self._update_step(grad, var)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\", line 233, in _update_step\n        raise KeyError(\n\n    KeyError: 'The optimizer cannot recognize variable dense_24/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.SGD.'\n"]}]},{"cell_type":"markdown","source":["The model converged when using 0.01 as the learning rate. However, it diverged when using 100, because that value deemed to be too large. Using 0.0000001 as the learning rate causes the model to improve very slowly. Hence the accuracy improved very little. So, it deemed to be too low."],"metadata":{"id":"s4_ZCxLnGHrC"}},{"cell_type":"markdown","source":["sample solution:\n","https://colab.research.google.com/drive/1Tux1Bb6FV_8_DfoqKztiTbhevdphoUzh#scrollTo=bODT_kzYs5uA"],"metadata":{"id":"shdGU_8rFgOI"}}]}